{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)\nThis notebook handles all of our data cleansing, feature engineering and table merges. These results are passed via pickle into the model notebook.\n\nhttps://www.kaggle.com/cloycebox/default-risk-week-6-models/edit","metadata":{}},{"cell_type":"markdown","source":"### Import packages\nThe Switch variable (in the first line of code) tells whether to create the training (0) or test (1) data sets. FNs array contains the names of the pickle files that we will save our outputs to.\n\nI considered including the test / validation split and pipeline in this notebook as well, so that the outputs of this file could be X_train, X_valid, X_test, y_train and y_valid (there is no such thing as y_test of course.) Ultimately I decided against this because the most time-consuming part of the whole process is uploading the test and train pickle files into the model notebook - it's >1GB of data and can take 10-15 minutes to upload. Running it through the pipeline and creating dummy columns would only make the data set larger and file transfer slower. So the first steps of the model notebook are reading the pickle files from this notebook and then applying train/valid split and pipeline to them.","metadata":{}},{"cell_type":"code","source":"Switch = 1                                                       # 0 creates training data set, 1 creates test data set\nDFs = ['application_train.csv', 'application_test.csv']          # which data frame to load, based on switch\nFNs = ['train1205.pkl', 'test1205.pkl']                          # what to name pickle output, based on switch\n\nimport numpy as np\nimport pandas as pd\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \nimport pickle\n\n# clear out any old junk\ngc.collect()\n\n# define the directory where our stuff lives\nMainDir = \"../input/../input/home-credit-default-risk\"\nprint(os.listdir(MainDir))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:33:27.580199Z","iopub.execute_input":"2021-12-07T22:33:27.580851Z","iopub.status.idle":"2021-12-07T22:33:27.692421Z","shell.execute_reply.started":"2021-12-07T22:33:27.580807Z","shell.execute_reply":"2021-12-07T22:33:27.691686Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"['sample_submission.csv', 'bureau_balance.csv', 'POS_CASH_balance.csv', 'application_train.csv', 'HomeCredit_columns_description.csv', 'application_test.csv', 'previous_application.csv', 'credit_card_balance.csv', 'installments_payments.csv', 'bureau.csv']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Building the training / test data set\nStrategy:\n* merge bureau_balance into bureau using SK_ID_BUREAU key, then merge bureau into train/test using SK_ID_CURR key\n* all other tables will be merged directly into train/test using SK_ID_CURR key (not going to use SK_ID_PREV)\n\nI also tried two-step merging, e.g. merging installment_payments into previous_application on SK_ID_PREV, and then merging previous_application into the main data set on SK_ID_CURR. I did not find any appreciable difference between doing it that way and directly merging installment_payments into main on SK_ID_CURR, so I decided to do it the direct way for the final data sets.\n\nFinal output tables will be saved to train.pkl and test.pkl","metadata":{}},{"cell_type":"markdown","source":"### bureau_balance.csv\nload bureau_balance.csv, make two pivot tables (one for overall, one for last 12 months only) - all we can do with them is merge them into bureau using SK_ID_BUREAU because we are not given SK_ID_CURR in this table.","metadata":{}},{"cell_type":"code","source":"bureau_balance = pd.read_csv(f'{MainDir}/bureau_balance.csv')                                           # read the table\nprint(bureau_balance.shape, \"- shape of bureau_balance table\")\nbb_status = pd.crosstab(bureau_balance.SK_ID_BUREAU, bureau_balance.STATUS, margins = True)             # create pivot table\nbb_status.columns = ['BB_'+column for column in bb_status.columns]\nprint(bb_status.shape, \"- shape of bb_status table\")\nbureau_balance_12 = bureau_balance[bureau_balance.MONTHS_BALANCE > -24]                                 # filter - last 12 months only\nbb_status_12 = pd.crosstab(bureau_balance_12.SK_ID_BUREAU, bureau_balance_12.STATUS, margins = True)    # pivot table on filtered data\nbb_status_12.columns = ['BB12_'+column for column in bb_status_12.columns]\nprint(bb_status_12.shape, \"- shape of bb_status_12 table\")\n\ndel bureau_balance            # we are keeping bb_status and bb_status_12 - we can drop the original table and the filtered version\ndel bureau_balance_12\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:36:58.779266Z","iopub.execute_input":"2021-12-07T22:36:58.779543Z","iopub.status.idle":"2021-12-07T22:37:55.600150Z","shell.execute_reply.started":"2021-12-07T22:36:58.779514Z","shell.execute_reply":"2021-12-07T22:37:55.599294Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(27299925, 3) - shape of bureau_balance table\n(817396, 9) - shape of bb_status table\n(721535, 9) - shape of bb_status_12 table\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### bureau.csv\nwe will create bureau_num, bureau_cat and bureau_count - these will later be merged into train and into test","metadata":{}},{"cell_type":"code","source":"bureau = pd.read_csv(f'{MainDir}/bureau.csv')\nprint(bureau.shape, \"- shape of bureau table\")\n\nbureau = bureau.merge(bb_status, left_on = 'SK_ID_BUREAU', right_on = 'SK_ID_BUREAU')               # merge the tables\nbureau = bureau.merge(bb_status_12, left_on = 'SK_ID_BUREAU', right_on = 'SK_ID_BUREAU')            # merge the tables\nbureau = bureau.drop(['SK_ID_BUREAU'], axis = 1)                                                    # no longer need this key\nprint(bureau.shape, \"- shape of bureau table after merging in bb_status tables\")                    # should be 652,144 x 34\nbureau.columns = ['BU_'+column if column !='SK_ID_CURR' else column for column in bureau.columns]   # things that start with BU_ came from bureau\n\n# ratio features\nbureau['OD_ratio'] = bureau['BU_AMT_CREDIT_SUM_OVERDUE'] / bureau['BU_AMT_CREDIT_SUM_DEBT']   # proportion of debt that is overdue\nbureau['Credit_ratio'] = bureau['BU_AMT_CREDIT_SUM'] / bureau['BU_AMT_CREDIT_SUM_LIMIT']      # proportion of credit line used\nbureau['Debt_ratio'] = bureau['BU_AMT_CREDIT_SUM_DEBT'] / bureau['BU_AMT_CREDIT_SUM']         # debt percentage\nbureau['REMAIN_CRED'] = bureau['BU_AMT_CREDIT_SUM'] - bureau['BU_AMT_CREDIT_SUM_DEBT'] - bureau['BU_AMT_CREDIT_SUM_LIMIT']\nbureau['AC_RATIO'] = bureau['BU_AMT_ANNUITY'] / bureau['BU_AMT_CREDIT_SUM']\n\n# numeric features for bureau\nbureau_num = bureau.groupby(by=['SK_ID_CURR']).mean().reset_index()                                 # group the numeric features by SK_ID_CURR\nprint(bureau_num.shape, \"- shape of numeric bureau features (incl index)\")                          # should be 132,250 x 34\n\n# categorical feagures for bureau\nbureau_cat = pd.get_dummies(bureau.select_dtypes('object'))                                         # this got rid of the SK_ID_CURR column ...\nbureau_cat['SK_ID_CURR'] = bureau['SK_ID_CURR']                                                     # so we have to replace it\nbureau_cat = bureau_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()                           # tried sum - didn't change anything\nprint(bureau_cat.shape, \"- shape of categorical bureau features (incl index)\")                      # should be 132,250 x 23\n\n# count feature for bureau\nbureau_count = bureau.groupby(by = ['SK_ID_CURR'])['BU_CREDIT_ACTIVE'].count().reset_index()\nbureau_count.rename(columns={'BU_CREDIT_ACTIVE':'COUNT_of_BUREAU'})   \n\n# we are keeping bureau_num, bureau_cat and bureau_count - all will be merged into training data\ndel bureau                   # no longer need this table - its contents were transformed into bureau_num, bureau_cat, bureau_count\ndel bb_status                # no longer need this table - already merged into bureau\ndel bb_status_12             # no longer need this table - already merged into bureau\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:34:25.545378Z","iopub.execute_input":"2021-12-07T22:34:25.545600Z","iopub.status.idle":"2021-12-07T22:34:32.230420Z","shell.execute_reply.started":"2021-12-07T22:34:25.545573Z","shell.execute_reply":"2021-12-07T22:34:32.229492Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(1716428, 17) - shape of bureau table\n(693550, 34) - shape of bureau table after merging in bb_status tables\n(133231, 36) - shape of numeric bureau features (incl index)\n(133231, 23) - shape of categorical bureau features (incl index)\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### previous_application.csv\nwe will create previous_num and previous_cat - these will later be merged into train and into test.\n\nIt takes 7-8 minutes to calculate the CALC_RATE feature for all 1.6 million unique SK_ID_PREVs. So I ran them once, saved the output to PR_calcs.pkl, and then retrieve that information as needed, instead of recalculating every time.","metadata":{}},{"cell_type":"code","source":"# This is the original function that derives CALC_RATE, INTEREST_PAID and INT_PRINC.\n\n# def calc_rate(row):\n#    return np.rate(row['CNT_PAYMENT'], -row['AMT_ANNUITY'], row['AMT_CREDIT'], 0, guess = 0.05, maxiter = 10)\n# previous['CALC_RATE'] = previous.apply(calc_rate, axis=1)\n# previous['INTEREST_PAID'] = previous['AMT_ANNUITY'] * previous['CNT_PAYMENT'] - previous['AMT_CREDIT']\n# previous['INT_PRINC'] = previous['INTEREST_PAID'] / previous['AMT_CREDIT']\n\nAPR = train = pd.read_pickle('../input/aprdata/PR_calcs.pkl')\nAPR.drop(['SK_ID_CURR'], axis = 1, inplace = True)\nAPR.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:34:32.232560Z","iopub.execute_input":"2021-12-07T22:34:32.232778Z","iopub.status.idle":"2021-12-07T22:34:32.293807Z","shell.execute_reply.started":"2021-12-07T22:34:32.232752Z","shell.execute_reply":"2021-12-07T22:34:32.292941Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"   SK_ID_PREV  CALC_RATE  INTEREST_PAID  INT_PRINC\n0     2030495   0.030778        3620.16   0.211150\n1     2802425   0.016497      227119.14   0.334160\n2     2523466   0.046136       44284.32   0.324559\n3     2819243   0.029095       93706.02   0.199040\n4     1784265   0.059090      362130.48   0.896241","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_PREV</th>\n      <th>CALC_RATE</th>\n      <th>INTEREST_PAID</th>\n      <th>INT_PRINC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2030495</td>\n      <td>0.030778</td>\n      <td>3620.16</td>\n      <td>0.211150</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2802425</td>\n      <td>0.016497</td>\n      <td>227119.14</td>\n      <td>0.334160</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2523466</td>\n      <td>0.046136</td>\n      <td>44284.32</td>\n      <td>0.324559</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2819243</td>\n      <td>0.029095</td>\n      <td>93706.02</td>\n      <td>0.199040</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1784265</td>\n      <td>0.059090</td>\n      <td>362130.48</td>\n      <td>0.896241</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\nprevious = pd.read_csv(f'{MainDir}/previous_application.csv')\nprint(previous.shape)\nprevious = previous.merge(APR, on='SK_ID_PREV', how='left')                      # the interest rate info that we just loaded\nprint(previous.shape)\n\nprevious.drop(['SK_ID_PREV'], axis = 1, inplace = True)                          # don't need this, merging everything straight to train/test\n\nprevious.columns = ['PR_'+column if column !='SK_ID_CURR' else column for column in previous.columns]\nprevious['PR_DAYS_LAST_DUE'].replace({365243: np.nan}, inplace = True)\nprevious['PR_DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace = True)\nprevious['PR_DAYS_FIRST_DUE'].replace(365243, np.nan, inplace = True)\nprevious['PR_DAYS_TERMINATION'].replace({365243: np.nan}, inplace = True)\nprevious['PR_DAYS_FIRST_DRAWING'].replace({365243: np.nan}, inplace = True)\nprevious['PR_CApp_RATIO'] = previous['PR_AMT_CREDIT'] / previous['PR_AMT_APPLICATION'] \nprevious['PR_AG_RATIO'] = previous['PR_AMT_APPLICATION'] / previous['PR_AMT_GOODS_PRICE'] \nprevious['PR_CAnnRATIO'] = previous['PR_AMT_CREDIT'] / previous['PR_AMT_ANNUITY'] \nprevious['PR_CG_RATIO'] = previous['PR_AMT_CREDIT'] / previous['PR_AMT_GOODS_PRICE'] \n\n# Create numeric features by grouping on SK_ID_CURR and finding group means\nprevious_num = previous.groupby(by=['SK_ID_CURR']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(previous_num.shape, \"- shape of numeric features (incl index)\")         \n\n# Create categorical features by creating dummies and then taking group means\nprevious_cat = pd.get_dummies(previous.select_dtypes('object'))                 # this got rid of the SK_ID_CURR column ...\nprevious_cat['SK_ID_CURR'] = previous['SK_ID_CURR']                             # so we have to replace it\nprevious_cat = previous_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()   # could try sum as well.\nprint(previous_cat.shape, \"- shape of categorical features (incl index)\")     \n\n# we can keep previous_num and previous_cat - these will be merged into training data\ndel previous                 # no longer need this table - its contents were transformed into previous_num, previous_cat\ndel APR                      # already merged into previous\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:34:32.294944Z","iopub.execute_input":"2021-12-07T22:34:32.295147Z","iopub.status.idle":"2021-12-07T22:34:57.872519Z","shell.execute_reply.started":"2021-12-07T22:34:32.295121Z","shell.execute_reply":"2021-12-07T22:34:57.871613Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(1670214, 37)\n(1670214, 40)\n(338857, 27) - shape of numeric features (incl index)\n(338857, 144) - shape of categorical features (incl index)\nCPU times: user 20.3 s, sys: 5.26 s, total: 25.6 s\nWall time: 25.6 s\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### installment_payments.csv\nonly contains numeric features. we will create inst_num, which will later be merged into train and test","metadata":{}},{"cell_type":"code","source":"inst = pd.read_csv(f'{MainDir}/installments_payments.csv')\ninst.drop(['SK_ID_PREV'], axis=1, inplace = True)\ninst.columns = ['IP_'+column if column != 'SK_ID_CURR' else column for column in inst.columns]\n\ninst['PAY_PERCENT'] = inst['IP_AMT_INSTALMENT'] / inst['IP_AMT_PAYMENT']\ninst['PAY_DIFF'] = inst['IP_AMT_INSTALMENT'] - inst['IP_AMT_PAYMENT']\ninst['DPD'] = inst['IP_DAYS_ENTRY_PAYMENT'] - inst['IP_DAYS_INSTALMENT']\ninst['DPD'] = inst['DPD'].apply(lambda x: x if x>0 else 0)\ninst['DBD'] = inst['IP_DAYS_INSTALMENT'] - inst['IP_DAYS_ENTRY_PAYMENT']\ninst['DBD'] = inst['DBD'].apply(lambda x: x if x>0 else 0)\n\ninst_num = inst.groupby(by=['SK_ID_CURR']).mean().reset_index()         # group the numeric features by SK_ID_CURR\nprint(inst_num.shape, \"- shape of numeric features (incl index)\")       # should be 339,587 x 7\n\n# we will keep inst_num and get rid of inst\ndel inst                    # don't need this anymore\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:34:57.873707Z","iopub.execute_input":"2021-12-07T22:34:57.873918Z","iopub.status.idle":"2021-12-07T22:35:31.044627Z","shell.execute_reply.started":"2021-12-07T22:34:57.873891Z","shell.execute_reply":"2021-12-07T22:35:31.043601Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(339587, 11) - shape of numeric features (incl index)\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"#### POS_CASH_balance.csv\nwe will create pos_num and pos_cat - these will later be merged into train and into test","metadata":{}},{"cell_type":"code","source":"pos = pd.read_csv(f'{MainDir}/POS_CASH_balance.csv')\npos.drop(['SK_ID_PREV'], axis=1, inplace = True)\npos.columns = ['PO_'+column if column != 'SK_ID_CURR' else column for column in pos.columns]\n\npos_num = pos.groupby(by=['SK_ID_CURR']).mean().reset_index()            # group the numeric features by SK_ID_CURR\nprint(pos_num.shape, \"- shape of numeric features (incl index)\")         # should be 337,252 x 6\n\npos_cat = pd.get_dummies(pos.select_dtypes('object'))                    # this got rid of the SK_ID_CURR column ...\npos_cat['SK_ID_CURR'] = pos['SK_ID_CURR']                                # so we have to replace it\npos_cat = pos_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()      # could try sum as well.\nprint(pos_cat.shape, \"- shape of categorical features (incl index)\")     # should be 337,252 x 10\n\n# we will keep pos_num and pos_cat\ndel pos\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:35:31.046117Z","iopub.execute_input":"2021-12-07T22:35:31.046352Z","iopub.status.idle":"2021-12-07T22:35:44.647193Z","shell.execute_reply.started":"2021-12-07T22:35:31.046325Z","shell.execute_reply":"2021-12-07T22:35:44.646403Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(337252, 6) - shape of numeric features (incl index)\n(337252, 10) - shape of categorical features (incl index)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### credit_card_balance.csv","metadata":{}},{"cell_type":"code","source":"ccb = pd.read_csv(f'{MainDir}/credit_card_balance.csv')\nccb.drop(['SK_ID_PREV'], axis=1, inplace = True)\nccb.columns = ['CC_'+column if column != 'SK_ID_CURR' else column for column in ccb.columns]\n\nccb['DRAW_RATIO'] = ccb['CC_AMT_DRAWINGS_CURRENT'] / ccb['CC_CNT_DRAWINGS_CURRENT']\nccb['RECEIVE_RATIO'] = ccb['CC_AMT_RECIVABLE'] / ccb['CC_AMT_RECEIVABLE_PRINCIPAL']\nccb['RECEIVE_PER'] = ccb['CC_AMT_RECIVABLE'] / ccb['CC_AMT_TOTAL_RECEIVABLE']\n\nccb_num = ccb.groupby(by=['SK_ID_CURR']).mean().reset_index()            # group the numeric features by SK_ID_CURR\nprint(ccb_num.shape, \"- shape of numeric features (incl index)\")         # should be 103,558 x 24\n\nccb_cat = pd.get_dummies(ccb.select_dtypes('object'))                    # this got rid of the SK_ID_CURR column ...\nccb_cat['SK_ID_CURR'] = ccb['SK_ID_CURR']                                # so we have to replace it\nccb_cat = ccb_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()      # could try sum as well.\nprint(ccb_cat.shape, \"- shape of categorical features (incl index)\")     # should be 103,558 x 8\n\n# we will keep ccb_num and ccb_cat\ndel ccb\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:35:44.648841Z","iopub.execute_input":"2021-12-07T22:35:44.649224Z","iopub.status.idle":"2021-12-07T22:35:57.573830Z","shell.execute_reply.started":"2021-12-07T22:35:44.649159Z","shell.execute_reply":"2021-12-07T22:35:57.572852Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(103558, 24) - shape of numeric features (incl index)\n(103558, 8) - shape of categorical features (incl index)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### Treatment of main data set (train or test)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T03:37:04.083609Z","iopub.execute_input":"2021-12-02T03:37:04.083911Z","iopub.status.idle":"2021-12-02T03:37:04.088459Z","shell.execute_reply.started":"2021-12-02T03:37:04.08388Z","shell.execute_reply":"2021-12-02T03:37:04.087582Z"}}},{"cell_type":"code","source":"main = pd.read_csv(f'{MainDir}/{DFs[Switch]}')                          # either application_train or application_test depending on switch\n\n# column drops - Megan suggested many others\nmain.drop(['ORGANIZATION_TYPE'], axis = 1, inplace = True)              # creates 58 dummies that don't do jack shit - let's get rid of this\n\n# clean up this mess\nmain['FLAG_365243'] = main['DAYS_EMPLOYED'] == 365243                   # these people are mostly pensioners\nmain['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)         # DAYS_EMPLOYED is a powerful feature when you clean up the junk\n\n# ratio features\nmain['CI_ratio'] = main['AMT_CREDIT'] / main['AMT_INCOME_TOTAL']        # credit-to-income ratio\nmain['AI_ratio'] = main['AMT_ANNUITY'] / main['AMT_INCOME_TOTAL']       # annuity-to-income ratio\nmain['AC_ratio'] = main['AMT_CREDIT'] / main['AMT_ANNUITY']             # credit to annuity - basically the term of the loan in years\nmain['CG_ratio'] = main['AMT_CREDIT'] / main['AMT_GOODS_PRICE']         # credit to goods price ratio - how much was financed?\n\n# log features\nmain['log_INCOME'] = np.log(main['AMT_INCOME_TOTAL'])                    # log of income\nmain['log_ANNUITY'] = np.log(main['AMT_ANNUITY'])                        # log of annuity\nmain['log_CREDIT'] = np.log(main['AMT_CREDIT'])                          # log of credit\nmain['log_GOODS'] = np.log(main['AMT_GOODS_PRICE'])                      # log of goods price\n\n# flag features\nmain['FLAG_CG_ratio'] = main['AMT_CREDIT'] > main['AMT_GOODS_PRICE']     # FLAG if you borrowed more than the price of the item\nmain['DAYS_ID_4200'] = main['DAYS_ID_PUBLISH'] < -4200                   # IDs more than about 14 years old are from USSR\n\n# cleanup the ext_sources - and remember, these are columns 40:42 in the training data but 39:41 in the test data!\n# objective of this section is to replace missing scores with the ROW average for the scores we do have, and not impute any scores.\n# if you don't have ANY scores, you get a score of 0.2 - but there are only about a dozen of these\nmain['AVG_EXT'] = main.iloc[:,(40-Switch):(43-Switch)].sum(axis=1)/(3- main.iloc[:,(40-Switch):(43-Switch)].isnull().sum(axis=1))\nmain['AVG_EXT'].replace(np.nan, 0.2, inplace = True)       \nmain.EXT_SOURCE_1.fillna(main.AVG_EXT, inplace=True)\nmain.EXT_SOURCE_2.fillna(main.AVG_EXT, inplace=True)\nmain.EXT_SOURCE_3.fillna(main.AVG_EXT, inplace=True)\nmain['EXT_SOURCE_MAX'] = np.max(main.iloc[:,(40-Switch):(43-Switch)], axis = 1)\nmain['EXT_SOURCE_MIN'] = np.min(main.iloc[:,(40-Switch):(43-Switch)], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:35:57.575411Z","iopub.execute_input":"2021-12-07T22:35:57.575661Z","iopub.status.idle":"2021-12-07T22:35:58.473014Z","shell.execute_reply.started":"2021-12-07T22:35:57.575632Z","shell.execute_reply":"2021-12-07T22:35:58.472045Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# merge features\nDFs_to_merge = [bureau_cat, bureau_count, bureau_num, ccb_cat, ccb_num,\n inst_num, pos_cat, pos_num, previous_cat, previous_num]\n\nfor i in range (0, len(DFs_to_merge)):\n    main = main.merge(DFs_to_merge[i], on='SK_ID_CURR', how='left')\n\nprint(main.shape, \"- shape of data after all merges\") \n\n# get rid of any +/- infinity we might have missed\nmain.replace([np.inf, -np.inf], 0, inplace = True)\n\ndel bureau_cat\ndel bureau_count\ndel bureau_num\ndel ccb_cat\ndel ccb_num\ndel inst_num\ndel pos_cat\ndel pos_num\ndel previous_cat\ndel previous_num\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:35:58.475506Z","iopub.execute_input":"2021-12-07T22:35:58.475817Z","iopub.status.idle":"2021-12-07T22:36:00.171680Z","shell.execute_reply.started":"2021-12-07T22:35:58.475776Z","shell.execute_reply":"2021-12-07T22:36:00.170863Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"(48744, 415) - shape of data after all merges\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"markdown","source":"### Export our data set to pickle","metadata":{}},{"cell_type":"code","source":"main.to_pickle(f'./{FNs[Switch]}', compression='infer', storage_options=None)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T22:36:00.172903Z","iopub.execute_input":"2021-12-07T22:36:00.173521Z","iopub.status.idle":"2021-12-07T22:36:00.493050Z","shell.execute_reply.started":"2021-12-07T22:36:00.173487Z","shell.execute_reply":"2021-12-07T22:36:00.492287Z"},"trusted":true},"execution_count":22,"outputs":[]}]}